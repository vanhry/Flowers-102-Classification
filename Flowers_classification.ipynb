{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flowers_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "74Zj3EqBhrbH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This cell for use in Google Colab\n",
        "# But you can do this in Linux too\n",
        "\n",
        "!rm -r *\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\n",
        "!tar zxvf 102flowers.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OpafSy9miMZS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "TRAIN_DIR = 'jpg/'\n",
        "IMG_SIZE = 128\n",
        "batch_size = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v2vXCwkMiXf-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mat = scipy.io.loadmat('imagelabels.mat')\n",
        "labels = mat['labels']\n",
        "labels_name_st = ['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', 'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly', 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy', 'common dandelion', 'petunia', 'wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia?', 'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily', 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen ', 'watercress', 'canna lily', 'hippeastrum ', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily']\n",
        "labels_name = [i.replace(' ','_') for i in labels_name_st] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-wGKNu3dJN5_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_ind(filename, start=\"_0\",end=\".\"):\n",
        "    \"\"\"\n",
        "    Take substring with index of jpg-file\n",
        "    \n",
        "    -example: 'image_00345.jpg' -> 345\n",
        "    \"\"\"\n",
        "    filename = filename[filename.find(start) + len(start): filename.rfind(end)]\n",
        "    return int(filename) - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "11gvoJH_vxE-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rename_files(TRAIN_DIR, labels, names):\n",
        "    \"\"\"\n",
        "    Rename filenames in directory according to class of this image\n",
        "    \"\"\"\n",
        "    labels = np.squeeze(labels) # from [[1,2,3]] to [1,2,3]\n",
        "    jpg = '.jpg'\n",
        "    for i, filename in enumerate(os.listdir(TRAIN_DIR)):\n",
        "        label = labels[get_ind(os.path.join(TRAIN_DIR, filename))]\n",
        "        os.rename(os.path.join(TRAIN_DIR,filename), os.path.join(TRAIN_DIR, str(i+1) + '.' + names[label-1] + jpg))\n",
        "        \n",
        "        \n",
        "rename_files(TRAIN_DIR, labels,labels_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0U49HVUb8Nar",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_class_folders(TRAIN_DIR, names):\n",
        "    \"\"\"\n",
        "    Create folder for each class and move suit file to it\n",
        "    \"\"\"\n",
        "    for dirname in names:\n",
        "        os.mkdir(os.path.join(TRAIN_DIR, dirname))\n",
        "    for filename in os.listdir(TRAIN_DIR):\n",
        "        if filename.endswith('.jpg'):\n",
        "            suit_dir = filename.split('.')[1] # like this -  111.suit_dir.jpg\n",
        "            os.rename(os.path.join(TRAIN_DIR,filename), os.path.join(TRAIN_DIR, suit_dir, filename))\n",
        "\n",
        "create_class_folders(TRAIN_DIR, labels_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FQ9Y0VOTxn8j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "mwmzqjHShH8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "9b927d21-be05-419a-9688-323ae35458f3"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wlVxdbJthIZ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_finetune_model(dim, dropout, num_classes):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3),activation='relu', input_shape=(dim, dim, 3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Conv2D(64, (3, 3),activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    \n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())  # go from 3D to 1D\n",
        "    model.add(Dense(1024))  # Fully connected layer\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(dropout))  # dropout to avoid over-fitting from: https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
        "    model.add(Dense(num_classes))  # fully connected output layer\n",
        "    model.add(Activation('softmax'))  # softmax the output within the range of (0 to 1) for prediction capabilities\n",
        "\n",
        "    #  This compiles the model architecture and the necessary functions that we\n",
        "    #  categorical crossentropy is the loss function for classification problems with more than 2 classes\n",
        "\n",
        "    return model\n",
        " \n",
        "\n",
        "finetune_model = build_finetune_model(IMG_SIZE, dropout=0.33, \n",
        "                                      num_classes=102)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CICD1mrXxi7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a8debd7a-f904-4688-f7a0-caa9ef02cba5"
      },
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2) # set validation split\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training') # set as training data\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR, # same directory as training data\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # set as validation data"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6587 images belonging to 102 classes.\n",
            "Found 1602 images belonging to 102 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-ow9x28Qtv4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "6b328a9a-19dd-4143-f04f-55d835c5390e"
      },
      "cell_type": "code",
      "source": [
        "finetune_model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='rmsprop',  # a very nice optimization function with an adaptable learning rate\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "history = finetune_model.fit_generator(train_generator, epochs=30, workers=1, \n",
        "                                       steps_per_epoch=6500 // batch_size, \n",
        "                                       shuffle=True)\n",
        "\n",
        "\n",
        "plot_training(history)\n",
        "\n",
        "def plot_training(history):\n",
        "\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(len(acc))\n",
        "    plt.plot(epochs, acc, 'r.')\n",
        "    plt.plot(epochs, val_acc, 'r')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.show()\n",
        "    plt.savefig('acc_vs_epochs.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "406/406 [==============================] - 613s 2s/step - loss: 3.4801 - acc: 0.2195\n",
            "Epoch 2/30\n",
            "406/406 [==============================] - 607s 1s/step - loss: 2.2616 - acc: 0.4404\n",
            "Epoch 3/30\n",
            "406/406 [==============================] - 606s 1s/step - loss: 1.7575 - acc: 0.5498\n",
            "Epoch 4/30\n",
            "406/406 [==============================] - 623s 2s/step - loss: 1.4844 - acc: 0.6136\n",
            "Epoch 5/30\n",
            "406/406 [==============================] - 614s 2s/step - loss: 1.1826 - acc: 0.6798\n",
            "Epoch 6/30\n",
            "406/406 [==============================] - 595s 1s/step - loss: 1.0326 - acc: 0.7229\n",
            "Epoch 7/30\n",
            "406/406 [==============================] - 590s 1s/step - loss: 0.8856 - acc: 0.7617\n",
            "Epoch 8/30\n",
            "406/406 [==============================] - 590s 1s/step - loss: 0.8095 - acc: 0.7777\n",
            "Epoch 9/30\n",
            "406/406 [==============================] - 597s 1s/step - loss: 0.6957 - acc: 0.8077\n",
            "Epoch 10/30\n",
            "406/406 [==============================] - 607s 1s/step - loss: 0.6517 - acc: 0.8208\n",
            "Epoch 11/30\n",
            "406/406 [==============================] - 597s 1s/step - loss: 0.5962 - acc: 0.8372\n",
            "Epoch 12/30\n",
            "  2/406 [..............................] - ETA: 9:50 - loss: 0.3315 - acc: 0.9375"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}